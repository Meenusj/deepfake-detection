{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfkHfSr6Wra1VkxRJSlQw0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meenusj/deepfake-detection/blob/main/training_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn nltk fasttext tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlJXC-kZ-cY1",
        "outputId": "e1d667de-2cde-4fd9-ebf3-f11096338db2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199773 sha256=f5a28443d405c172c6d04c49e75bf0178683740acef5e9b59ca5c7e80fd791c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc4JN7JR84YD",
        "outputId": "a6eb9c83-dfcd-4c9d-ca5f-e09ef77896f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['screen_name', 'text', 'account.type', 'class_type'], dtype='object')\n",
            "Preprocessed data saved to preprocessed_dataset.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4559368a177e>:99: DtypeWarning: Columns (0,1,2,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('df_with_vectors.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['screen_name', 'text', 'account.type', 'class_type',\n",
            "       'preprocessed_text', 'feature_0', 'feature_1', 'feature_2', 'feature_3',\n",
            "       'feature_4',\n",
            "       ...\n",
            "       'feature_290', 'feature_291', 'feature_292', 'feature_293',\n",
            "       'feature_294', 'feature_295', 'feature_296', 'feature_297',\n",
            "       'feature_298', 'feature_299'],\n",
            "      dtype='object', length=305)\n",
            "Epoch 1/5\n",
            "3631/3631 [==============================] - 38s 10ms/step - loss: 0.5588 - accuracy: 0.8710 - val_loss: 0.5573 - val_accuracy: 0.8678\n",
            "Epoch 2/5\n",
            "3631/3631 [==============================] - 32s 9ms/step - loss: 0.5447 - accuracy: 0.8712 - val_loss: 0.5573 - val_accuracy: 0.8678\n",
            "Epoch 3/5\n",
            "3631/3631 [==============================] - 33s 9ms/step - loss: 0.5442 - accuracy: 0.8712 - val_loss: 0.5589 - val_accuracy: 0.8678\n",
            "Epoch 4/5\n",
            "3631/3631 [==============================] - 33s 9ms/step - loss: 0.5440 - accuracy: 0.8712 - val_loss: 0.5574 - val_accuracy: 0.8678\n",
            "Epoch 5/5\n",
            "3631/3631 [==============================] - 34s 9ms/step - loss: 0.5441 - accuracy: 0.8712 - val_loss: 0.5573 - val_accuracy: 0.8678\n",
            "1009/1009 [==============================] - 3s 3ms/step\n",
            "Accuracy: 0.8749\n",
            "Precision: 0.7655\n",
            "Recall: 0.8749\n",
            "F1 Score: 0.8165\n",
            "Confusion Matrix:\n",
            "[[    0     0     0     0   614]\n",
            " [    0     0     0     0  2030]\n",
            " [    0     0     0     0   725]\n",
            " [    0     0     0     0   668]\n",
            " [    0     0     0     0 28235]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       614\n",
            "           1       0.00      0.00      0.00      2030\n",
            "           2       0.00      0.00      0.00       725\n",
            "           3       0.00      0.00      0.00       668\n",
            "           4       0.87      1.00      0.93     28235\n",
            "\n",
            "    accuracy                           0.87     32272\n",
            "   macro avg       0.17      0.20      0.19     32272\n",
            "weighted avg       0.77      0.87      0.82     32272\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Import NLTK and download 'punkt' and 'stopwords'\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import fasttext\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Dense\n",
        "\n",
        "# Load your dataset (replace 'train.csv' with your actual dataset file)\n",
        "# Load the CSV file\n",
        "csv_file_path = 'train.csv'\n",
        "df = pd.read_csv(csv_file_path, sep=';')\n",
        "\n",
        "\n",
        "# Print column names to identify the correct column\n",
        "print(df.columns)\n",
        "\n",
        "# Assuming the correct column name is 'text', replace 'tweet_text' with the actual name\n",
        "def preprocess(text):\n",
        "    # Tokenization\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Case conversion\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # Remove hashtags and usernames\n",
        "    tokens = [re.sub(r'#\\w+|@\\w+', '', word) for word in tokens]\n",
        "\n",
        "    # Remove punctuation\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "\n",
        "    # Stopword removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply pre-processing to the specified column in the DataFrame\n",
        "# Replace 'tweet_text' with the actual column name, which is 'text' in this case\n",
        "df['preprocessed_text'] = df['text'].apply(preprocess)\n",
        "\n",
        "# Save the preprocessed DataFrame to a new CSV file\n",
        "preprocessed_csv_path = 'preprocessed_dataset.csv'\n",
        "df.to_csv(preprocessed_csv_path, index=False)\n",
        "\n",
        "print(f\"Preprocessed data saved to {preprocessed_csv_path}\")\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "preprocessed_csv_path = 'preprocessed_dataset.csv'\n",
        "df = pd.read_csv(preprocessed_csv_path)\n",
        "\n",
        "# Check for NaN values and replace them with an empty string\n",
        "df['preprocessed_text'].fillna('', inplace=True)\n",
        "\n",
        "# Check for non-string values and convert them to strings\n",
        "df['preprocessed_text'] = df['preprocessed_text'].astype(str)\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [word_tokenize(text) for text in df['preprocessed_text']]\n",
        "\n",
        "# Save tokenized text to a text file (required format for FastText)\n",
        "with open('tokenized_text.txt', 'w') as file:\n",
        "    for tokens in tokenized_text:\n",
        "        file.write(\" \".join(tokens) + \"\\n\")\n",
        "\n",
        "# Train FastText model\n",
        "model = fasttext.train_unsupervised('tokenized_text.txt', model='skipgram', dim=300, epoch=10)\n",
        "\n",
        "# Save the model\n",
        "model.save_model('fasttext_model.bin')\n",
        "\n",
        "# Get word vectors for each token\n",
        "word_vectors = [model.get_word_vector(word) for tokens in tokenized_text for word in tokens]\n",
        "\n",
        "# Convert word vectors to DataFrame\n",
        "word_vectors_df = pd.DataFrame(word_vectors, columns=[f'feature_{i}' for i in range(300)])\n",
        "\n",
        "# Concatenate the original DataFrame with the word vectors DataFrame\n",
        "df_with_vectors = pd.concat([df, word_vectors_df], axis=1)\n",
        "\n",
        "# Save the DataFrame with additional columns for word vectors\n",
        "df_with_vectors.to_csv('df_with_vectors.csv', index=False)\n",
        "\n",
        "\n",
        "# Load your DataFrame without header\n",
        "df = pd.read_csv('df_with_vectors.csv')\n",
        "print(df.columns)\n",
        "\n",
        "# Extract feature columns (assuming they start from column 'feature_0')\n",
        "feature_columns = df.columns[df.columns.str.startswith('feature_')]\n",
        "\n",
        "# Extract features and labels\n",
        "X = df[feature_columns].values\n",
        "y = df['class_type']\n",
        "# Encode class labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['class_type'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test_actual = train_test_split(X,y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the input data to be compatible with Conv1D layer\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Define and compile your model\n",
        "model_cnn = Sequential()\n",
        "model_cnn.add(Conv1D(filters=64, kernel_size=7, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "model_cnn.add(GlobalMaxPooling1D())\n",
        "model_cnn.add(Dropout(0.5))\n",
        "model_cnn.add(Dense(32, activation='relu'))\n",
        "model_cnn.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_cnn.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs = model_cnn.predict(X_test)\n",
        "y_pred_classes = y_pred_probs.argmax(axis=-1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test_actual, y_pred_classes)\n",
        "precision = precision_score(y_test_actual, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_test_actual, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_test_actual, y_pred_classes, average='weighted')\n",
        "\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Create a confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test_actual, y_pred_classes)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Create a classification report\n",
        "class_report = classification_report(y_test_actual, y_pred_classes)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# Save the model including architecture, optimizer, and learned weights\n",
        "model_cnn.save('deepfake_model.h5')\n",
        "\n",
        "# Save the label encoder for future use\n",
        "with open('label_encoder.pkl', 'wb') as le_file:\n",
        "    pickle.dump(label_encoder, le_file)\n",
        "\n",
        "\n"
      ]
    }
  ]
}