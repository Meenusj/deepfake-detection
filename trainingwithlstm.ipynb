{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd4IAqTM41YLTeSAylWdIO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meenusj/deepfake-detection/blob/main/trainingwithlstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn nltk fasttext tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ6NhCBBH4V5",
        "outputId": "6cefa74b-2c07-49ab-a4f0-3539ec5d80ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199772 sha256=9826afd78d5b9314f12b74f374fee8fc422b5747dd6748bf2677d733000cbe02\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n5Ak5mUAK7T",
        "outputId": "43b4c4df-c4a0-4fca-ced6-048d352a45f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['screen_name', 'text', 'account.type', 'class_type'], dtype='object')\n",
            "Preprocessed data saved to preprocessed_dataset.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-12919ede39e0>:81: DtypeWarning: Columns (0,1,2,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('df_with_vectors.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "3228/3228 [==============================] - 26s 7ms/step - loss: 0.5585 - accuracy: 0.8714 - val_loss: 0.5595 - val_accuracy: 0.8670\n",
            "Epoch 2/20\n",
            "3228/3228 [==============================] - 23s 7ms/step - loss: 0.5422 - accuracy: 0.8718 - val_loss: 0.5616 - val_accuracy: 0.8670\n",
            "Epoch 3/20\n",
            "3228/3228 [==============================] - 26s 8ms/step - loss: 0.5418 - accuracy: 0.8718 - val_loss: 0.5615 - val_accuracy: 0.8670\n",
            "Epoch 4/20\n",
            "3228/3228 [==============================] - 25s 8ms/step - loss: 0.5418 - accuracy: 0.8718 - val_loss: 0.5596 - val_accuracy: 0.8670\n",
            "1009/1009 [==============================] - 3s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM Model Evaluation Metrics:\n",
            "Accuracy: 0.8749\n",
            "Precision: 0.7655\n",
            "Recall: 0.8749\n",
            "F1 Score: 0.8165\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense, Embedding\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "import fasttext\n",
        "import re\n",
        "\n",
        "# Load your dataset (replace 'train.csv' with your actual dataset file)\n",
        "csv_file_path = 'train.csv'\n",
        "df = pd.read_csv(csv_file_path, sep=';')\n",
        "\n",
        "# Print column names to identify the correct column\n",
        "print(df.columns)\n",
        "\n",
        "# Assuming the correct column name is 'text'\n",
        "def preprocess(text):\n",
        "    tokens = text.split()\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    tokens = [re.sub(r'#\\w+|@\\w+', '', word) for word in tokens]\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply pre-processing to the 'text' column\n",
        "df['preprocessed_text'] = df['text'].apply(preprocess)\n",
        "\n",
        "# Save the preprocessed DataFrame to a new CSV file\n",
        "preprocessed_csv_path = 'preprocessed_dataset.csv'\n",
        "df.to_csv(preprocessed_csv_path, index=False)\n",
        "print(f\"Preprocessed data saved to {preprocessed_csv_path}\")\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "df = pd.read_csv(preprocessed_csv_path)\n",
        "\n",
        "# Check for NaN values and replace them with an empty string\n",
        "df['preprocessed_text'].fillna('', inplace=True)\n",
        "\n",
        "# Check for non-string values and convert them to strings\n",
        "df['preprocessed_text'] = df['preprocessed_text'].astype(str)\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [word_tokenize(text) for text in df['preprocessed_text']]\n",
        "\n",
        "# Save tokenized text to a text file (required format for FastText)\n",
        "with open('tokenized_text.txt', 'w') as file:\n",
        "    for tokens in tokenized_text:\n",
        "        file.write(\" \".join(tokens) + \"\\n\")\n",
        "\n",
        "# Train FastText model\n",
        "model_fasttext = fasttext.train_unsupervised('tokenized_text.txt', model='skipgram', dim=300, epoch=10)\n",
        "\n",
        "# Save the FastText model\n",
        "model_fasttext.save_model('fasttext_model.bin')\n",
        "\n",
        "# Get word vectors for each token\n",
        "word_vectors = [model_fasttext.get_word_vector(word) for tokens in tokenized_text for word in tokens]\n",
        "\n",
        "# Convert word vectors to DataFrame\n",
        "word_vectors_df = pd.DataFrame(word_vectors, columns=[f'feature_{i}' for i in range(300)])\n",
        "\n",
        "# Concatenate the original DataFrame with the word vectors DataFrame\n",
        "df_with_vectors = pd.concat([df, word_vectors_df], axis=1)\n",
        "\n",
        "# Save the DataFrame with additional columns for word vectors\n",
        "df_with_vectors.to_csv('df_with_vectors.csv', index=False)\n",
        "\n",
        "# Load your DataFrame with word vectors and labels\n",
        "df = pd.read_csv('df_with_vectors.csv')\n",
        "\n",
        "# Assuming 'class_type' is your label column\n",
        "X = df[df.columns[df.columns.str.startswith('feature_')]].values\n",
        "y = df['class_type']\n",
        "\n",
        "# Encode class labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test_actual = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the input data to be compatible with LSTM layer\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "# Build the LSTM model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'))\n",
        "model_lstm.add(Dropout(0.5))\n",
        "model_lstm.add(Dense(32, activation='relu'))\n",
        "model_lstm.add(Dense(np.unique(y_encoded).shape[0], activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model_lstm.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_probs_lstm = model_lstm.predict(X_test)\n",
        "y_pred_classes_lstm = np.argmax(y_pred_probs_lstm, axis=-1)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_lstm = accuracy_score(y_test_actual, y_pred_classes_lstm)\n",
        "precision_lstm = precision_score(y_test_actual, y_pred_classes_lstm, average='weighted')\n",
        "recall_lstm = recall_score(y_test_actual, y_pred_classes_lstm, average='weighted')\n",
        "f1_lstm = f1_score(y_test_actual, y_pred_classes_lstm, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics for the LSTM model\n",
        "print(f\"LSTM Model Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_lstm:.4f}\")\n",
        "print(f\"Precision: {precision_lstm:.4f}\")\n",
        "print(f\"Recall: {recall_lstm:.4f}\")\n",
        "print(f\"F1 Score: {f1_lstm:.4f}\")\n",
        "\n",
        "# Save the LSTM model\n",
        "model_lstm.save('lstm_model.h5')\n",
        "\n",
        "# Save the label encoder for future use\n",
        "with open('label_encoder_lstm.pkl', 'wb') as le_file_lstm:\n",
        "    pickle.dump(label_encoder, le_file_lstm)\n",
        "\n",
        "# Load the saved LSTM model\n",
        "loaded_model_lstm = load_model('lstm_model.h5')\n",
        "\n",
        "# Load the saved label encoder\n",
        "with open('label_encoder_lstm.pkl', 'rb') as le_file_lstm:\n",
        "    loaded_label_encoder = pickle.load(le_file_lstm)\n",
        "\n",
        "# Predict using the loaded LSTM model\n",
        "# Note: You need to preprocess and vectorize new text data similar to the training data\n",
        "# Then, use the loaded model and label encoder for predictions\n",
        "# ...\n",
        "\n",
        "# Alternatively, you can reuse the preprocessing and vectorization code used for the training data\n",
        "# and then make predictions using the loaded model and label encoder\n",
        "# ...\n"
      ]
    }
  ]
}